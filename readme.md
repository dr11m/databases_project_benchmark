## В данном репозитории я планирую сравнить timescaleDB и mongoDB для чтения и записи


<details>
  <summary>два основных сценария для теста:</summary>

>эти сценарии и есть моя основная нагрузка и сложность внутри реального проекта, для решения и были написаны эти тесты:
>1) запись 50к строк, вплоть до 500кк строк 
>2) так как в таблице будет примерно 100к уникальных id, указывающих на имена предметов, то на 1 предмет будет приходится примерно 5к записей, тут я и хочу проверить, на сколько шустро я буду получать эти 5к записей для каждого предмета, по мере заполнения таблицы
</details>

---

<details>
<summary>порядок проведения теста:</summary>

- Создать контейнер в Docker для каждой базы данных
- Сформировать таблицу с колонками: item_id, order_price, sale_price, date_added
- Создать код в Python для сравнения скоростей выполнения двух наших сценариев (брать средние значения результатов, чтобы исключить выбросы и получить объективное сравнение), вот подробные сценарии тестов:
    1) Оценить, как меняется скорость записи по мере заполнения таблицы, при каждом добавлении 50к строк
    2) Оценить, как быстро он может запихивать эти 50к строк (это не нужно для проектной задачи, но интересно)
    3) На сколько быстро я могу получать данные для каждого уникального предмета, при полном заполнении таблицы таких возвращаемых строк для каждого уникального id должно быть примерно 5к, а уникальных предметов примерно 100к (получить предельную скорость получения для каждой БД)
    4) Сравнить эту скорость при разном заполнении таблицы, для 1кк строк, 100кк и предельном в 500кк строк
    5) Отрисовать графики скоростей в matplotlib и агрегировать информацию о результатах в файл
</details>

---

<details>
  <summary>как запустить тест:</summary>

</details>

>
---
>

<details>
  <summary>результаты теста:</summary>

</details>

#TODO разделить timescaledb на 4 таблицы и добавлять последовательно в каждую, на записи параллельно отправлять запрос в 4 таблицы